 Memecoin signals – tuning cheat sheet (brief)
 Updated: 2025-09-26

 1) Fast context – why alerts were 0
 - Safety unknowns blocked: mint/lp unknown with strict gates → score 0.
 - Activity too strict: vol/mcap ≥0.8 and vol24 ≥50k filtered many.
 - Cap gate too low: >400k rejected without strong 1h.
 - Velocity gate required >0 while most had velocity +0.
 - DexScreener often returned liquidity=0; any liq floor blocked.

 2) Flow settings (used to confirm pipeline works)
 - ALLOW_UNKNOWN_SECURITY=true
 - REQUIRE_MINT_REVOKED=false, REQUIRE_LP_LOCKED=false
 - CIELO_DISABLE_STATS=true (temporary)
 - MIN_LIQUIDITY_USD=0
 - VOL_TO_MCAP_RATIO_MIN=0.20
 - MAX_MARKET_CAP_FOR_DEFAULT_ALERT=5,000,000
 - HIGH_CONFIDENCE_SCORE=8
 - REQUIRE_SMART_MONEY_FOR_ALERT=false
 - REQUIRE_VELOCITY_MIN_SCORE_FOR_ALERT=0

 3) Balanced starting gates (current defaults in config)
 - HIGH_CONFIDENCE_SCORE=9
 - MIN_LIQUIDITY_USD=10,000
 - MAX_MARKET_CAP_FOR_DEFAULT_ALERT=1,500,000
 - VOL_TO_MCAP_RATIO_MIN=0.60
 - VOL_24H_MIN_FOR_ALERT=0  (use 50k–200k when tightening)
 - ALLOW_UNKNOWN_SECURITY=true (until Cielo stats are reliable)
 - REQUIRE_MINT_REVOKED=false, REQUIRE_LP_LOCKED=false (enable later)

 4) Tighten vs Relax ladders
 - Too many/low quality → tighten in order:
   1) MIN_LIQUIDITY_USD → 10k → 20k → 30k
   2) VOL_TO_MCAP_RATIO_MIN → 0.6 → 1.0 → 1.5
   3) VOL_24H_MIN_FOR_ALERT → 50k → 100k → 200k
   4) HIGH_CONFIDENCE_SCORE → 9 → 10
   5) Enable REQUIRE_MINT_REVOKED=true, REQUIRE_LP_LOCKED=true; then set ALLOW_UNKNOWN_SECURITY=false
 - Too few alerts → relax in order:
   1) HIGH_CONFIDENCE_SCORE 10→9→8
   2) VOL_TO_MCAP_RATIO_MIN 1.0→0.6→0.3
   3) VOL_24H_MIN_FOR_ALERT 100k→50k→30k
   4) TEMPORARILY set MIN_LIQUIDITY_USD 30k→20k→10k→0

 4a) Gate Mode (tiered presets via GATE_MODE)
 - TIER1 (High Confidence): score≥9, liq≥20k, vol24≥50k, mcap≤1.5M, vol/mcap≥0.6
 - TIER2 (Balanced Default): score≥9, liq≥10k, mcap≤1.5M, vol/mcap≥0.6
 - TIER3 (Exploratory): score≥8, liq≥5k, mcap≤5M, vol/mcap≥0.3
   Set with GATE_MODE=TIER1|TIER2|TIER3 in .env (overrides defaults).

 5) Data-backed predictors for ≥2x (earlier dataset)
 - Vol/MCap ≥ ~1.08
 - 1h momentum ≥ ~29%
 - Microcap at alert ≤ ~100k
 - Liquidity ≥ ~30k
 - 24h volume ≥ ~100k
 - Final score 10 correlated best with winners (small N)

 6) Safety policy sequence (enable as Cielo stabilizes)
 - Phase A (current while fallback in use): ALLOW_UNKNOWN_SECURITY=true; REQUIRE_* false
 - Phase B: CIELO_DISABLE_STATS=false; REQUIRE_MINT_REVOKED=true; REQUIRE_LP_LOCKED=true
 - Phase C: ALLOW_UNKNOWN_SECURITY=false; enforce MAX_TOP10_CONCENTRATION ≤40%

 7) Minimal diagnostics
 - Summary counters
   watch -n 2 'awk "/Database initialized successfully/{start=NR} NR>=start && /FETCHING DETAILED STATS/{det++} NR>=start && /FINAL:/{fin++} NR>=start && /Alert #[0-9]+ sent/{al++} END{printf \"prelim_pass=%d  finals=%d  alerts_sent=%d\n\", det+0, fin+0, al+0}" logs/stdout.log'
 - Score histogram
   THR=$(python - <<'PY'\nimport sys; sys.path.insert(0,'.'); import config; print(config.HIGH_CONFIDENCE_SCORE)\nPY\n); export THR
   awk '/Database initialized successfully/{start=NR} NR>=start && /FINAL:/ { if (match($0,/FINAL: ([0-9]+)/,m)) c[m[1]]++ } END{printf "HIGH_CONFIDENCE_SCORE=%s\n", ENVIRON["THR"]; for(i=0;i<=10;i++) printf "score_%d=%d%s", i, (c[i]+0), (i<10?"  ":"\n")}' logs/stdout.log
 - Live final-stage reasons
   tail -F logs/stdout.log | grep -E "FINAL:|Filtered:|Alert #[0-9]+ sent"

 8) Measure outcomes (quick exports)
 - jq -r 'select(.final_score) | [.ts,.token,.final_score,.vol_to_mcap_ratio,(.gates.passes|tostring)] | @csv' logs/alerts.jsonl > exports/alerts_export.csv
 - jq -r 'select(.peak_x_price) | [.token,.peak_x_price,.time_to_peak_price_s] | @csv' logs/tracking.jsonl > exports/tracking_export.csv

 9) Review cadence
 - Run for 12–24h per setting step; target ≥2x hit rate >5%.
 - If rugs persist, raise MIN_LIQUIDITY_USD (→40–50k) and lower MAX_TOP10_CONCENTRATION.

 9a) Expected ≥1.5x hit rates by tier (estimates; validate live)
 - TIER1: ~22–28% per alert
 - TIER2: ~15–20% per alert
 - TIER3: ~9–13% per alert
 Use TIER1 as baseline; step to TIER2 only after sustained ≥25% ≥1.5x rate.

 10) What is being tracked (ground truth + features)
 - DB tables
   - alerted_tokens(token_address PK, alerted_at, final_score, smart_money_detected)
   - alerted_token_stats(token_address PK, first/last price|mcap|liq|vol24, peak_* values and timestamps, last_checked_at, outcome, outcome_at, peak_drawdown_pct)
   - token_activity(id PK, token_address, observed_at, usd_value, transaction_count, smart_money_involved, preliminary_score, trader_address)
 - Logs
   - logs/alerts.jsonl: includes final_score, prelim_score, velocity details, vol_to_mcap_ratio, safety fields (mint_revoked, lp_locked/burned, top10_concentration_percent), raw objects (security/liquidity/holders), and gates {thresholds, passes}
   - logs/tracking.jsonl: periodic snapshots with price/mcap/liq/vol24 and peak multiples/timings

 11) Live monitors (copy/paste)
 - Assert “alert → DB baseline row” linkage as signals fire
   tail -F logs/stdout.log | stdbuf -oL grep -E "Alert #[0-9]+ sent for token" | awk '{print $NF}' | while read CA; do
     sqlite3 -header -column var/alerted_tokens.db "\
       SELECT token_address, first_price_usd, first_market_cap_usd, first_liquidity_usd,\
              last_price_usd, peak_price_usd, peak_market_cap_usd\
       FROM alerted_token_stats WHERE token_address='$CA';";
   done
 - Enriched alerts present check
   tail -n 50 logs/alerts.jsonl | jq -cr 'select(.gates and .mint_revoked!=null) | {token,score:.final_score,vol_to_mcap:.vol_to_mcap_ratio,passes:.gates.passes}' || tail -n 50 logs/alerts.jsonl

 12) Nightly dataset export for ML (writes /root/datasets/alerts_training_*.csv)
 - One-liner (cron-safe)
   bash -lc 'cd /opt/callsbotonchain && source venv/bin/activate && TS=$(date +%F_%H%M%S) && OUT=/root/datasets/alerts_training_$TS.csv && sqlite3 -header -csv var/alerted_tokens.db "
   WITH base AS (
     SELECT t.token_address, t.alerted_at, t.final_score, t.smart_money_detected,
            s.first_price_usd, s.first_market_cap_usd, s.first_liquidity_usd,
            s.last_price_usd, s.last_market_cap_usd, s.last_liquidity_usd, s.last_volume_24h_usd,
            s.peak_price_usd, s.peak_market_cap_usd, s.peak_volume_24h_usd,
            s.first_alert_at, s.peak_price_at, s.peak_market_cap_at,
            s.outcome, s.peak_drawdown_pct,
            ROUND(s.peak_price_usd/NULLIF(s.first_price_usd,0),4)  AS peak_x_price,
            ROUND(s.peak_market_cap_usd/NULLIF(s.first_market_cap_usd,0),4) AS peak_x_mcap,
            (strftime('%s',s.peak_price_at)-strftime('%s',s.first_alert_at))  AS time_to_peak_price_s,
            (strftime('%s',s.peak_market_cap_at)-strftime('%s',s.first_alert_at)) AS time_to_peak_mcap_s,
            CASE WHEN s.first_price_usd>0 AND s.peak_price_usd>=2.0*s.first_price_usd THEN 1 ELSE 0 END AS label_ge_2x_price,
            CASE WHEN s.first_market_cap_usd>0 AND s.peak_market_cap_usd>=2.0*s.first_market_cap_usd THEN 1 ELSE 0 END AS label_ge_2x_mcap
     FROM alerted_tokens t JOIN alerted_token_stats s USING(token_address)
   ),
   act15 AS (
     SELECT a.token_address, COUNT(*) AS obs_15m_after,
            COALESCE(AVG(a.usd_value),0) AS avg_usd_15m_after,
            SUM(CASE WHEN a.smart_money_involved=1 THEN 1 ELSE 0 END) AS sm_15m_after,
            COUNT(DISTINCT COALESCE(a.trader_address,'')) AS uniq_15m_after
     FROM token_activity a JOIN alerted_tokens t USING(token_address)
     WHERE a.observed_at BETWEEN t.alerted_at AND datetime(t.alerted_at,'+15 minutes')
     GROUP BY a.token_address
   )
   SELECT b.*, COALESCE(a.obs_15m_after,0) AS obs_15m_after,
                 COALESCE(a.avg_usd_15m_after,0) AS avg_usd_15m_after,
                 COALESCE(a.sm_15m_after,0) AS sm_15m_after,
                 COALESCE(a.uniq_15m_after,0) AS uniq_15m_after
   FROM base b LEFT JOIN act15 a ON a.token_address=b.token_address
   ORDER BY b.alerted_at;" > $OUT && echo WROTE $OUT'

 13) Morning checklist (quick)
 - Confirm bot is running
   pgrep -a -f "[s]cripts/bot.py"
 - Watch key events for 1–2 minutes
   tail -F logs/stdout.log | grep -E "FEED ITEMS|FETCHING DETAILED|FINAL:|Alert #[0-9]+ sent" | sed -n '1,120p'
 - Quick counters since startup
   watch -n 2 'awk "/Database initialized successfully/{start=NR} NR>=start && /FETCHING DETAILED STATS/{det++} NR>=start && /FINAL:/{fin++} NR>=start && /Alert #[0-9]+ sent/{al++} END{printf \"prelim_pass=%d  finals=%d  alerts_sent=%d\n\", det+0, fin+0, al+0}" logs/stdout.log'
 - Verify alerts write baseline rows
   tail -F logs/stdout.log | stdbuf -oL grep -E "Alert #[0-9]+ sent for token" | awk '{print $NF}' | head -n1 | xargs -I{} sqlite3 -header -column var/alerted_tokens.db "SELECT token_address, first_price_usd, first_market_cap_usd FROM alerted_token_stats WHERE token_address='{}';"
 - Export latest dataset
   TS=$(date +%F_%H%M%S); OUT=/root/datasets/alerts_training_$TS.csv; sqlite3 -header -csv var/alerted_tokens.db < exports/sql/training.sql > "$OUT" 2>/dev/null || { printf "Using inline SQL...\n"; sqlite3 -header -csv var/alerted_tokens.db > "$OUT" <<'SQL'\nWITH base AS (\n  SELECT t.token_address, t.alerted_at, t.final_score, t.smart_money_detected,\n         s.first_price_usd, s.first_market_cap_usd, s.first_liquidity_usd,\n         s.last_price_usd, s.last_market_cap_usd, s.last_liquidity_usd, s.last_volume_24h_usd,\n         s.peak_price_usd, s.peak_market_cap_usd, s.peak_volume_24h_usd,\n         s.first_alert_at, s.peak_price_at, s.peak_market_cap_at,\n         s.outcome, s.peak_drawdown_pct,\n         ROUND(s.peak_price_usd/NULLIF(s.first_price_usd,0),4) AS peak_x_price,\n         ROUND(s.peak_market_cap_usd/NULLIF(s.first_market_cap_usd,0),4) AS peak_x_mcap,\n         (strftime('%s',s.peak_price_at)-strftime('%s',s.first_alert_at)) AS time_to_peak_price_s,\n         (strftime('%s',s.peak_market_cap_at)-strftime('%s',s.first_alert_at)) AS time_to_peak_mcap_s,\n         CASE WHEN s.first_price_usd>0 AND s.peak_price_usd>=2.0*s.first_price_usd THEN 1 ELSE 0 END AS label_ge_2x_price,\n         CASE WHEN s.first_market_cap_usd>0 AND s.peak_market_cap_usd>=2.0*s.first_market_cap_usd THEN 1 ELSE 0 END AS label_ge_2x_mcap\n  FROM alerted_tokens t JOIN alerted_token_stats s USING(token_address)\n),\nact15 AS (\n  SELECT a.token_address, COUNT(*) AS obs_15m_after,\n         COALESCE(AVG(a.usd_value),0) AS avg_usd_15m_after,\n         SUM(CASE WHEN a.smart_money_involved=1 THEN 1 ELSE 0 END) AS sm_15m_after,\n         COUNT(DISTINCT COALESCE(a.trader_address,'')) AS uniq_15m_after\n  FROM token_activity a JOIN alerted_tokens t USING(token_address)\n  WHERE a.observed_at BETWEEN t.alerted_at AND datetime(t.alerted_at,'+15 minutes')\n  GROUP BY a.token_address\n)\nSELECT b.*, COALESCE(a.obs_15m_after,0) AS obs_15m_after,\n              COALESCE(a.avg_usd_15m_after,0) AS avg_usd_15m_after,\n              COALESCE(a.sm_15m_after,0) AS sm_15m_after,\n              COALESCE(a.uniq_15m_after,0) AS uniq_15m_after\nFROM base b LEFT JOIN act15 a ON a.token_address=b.token_address\nORDER BY b.alerted_at;\nSQL\n echo "WROTE $OUT"; }
 - If no alerts or all finals ~0 → use section 4 to relax; if noisy → tighten per ladder.

 14) $1k → six-figures plan (risk-based auto-trading)
 - Position sizing: risk 0.5–1.0% of equity per trade; size = risk_usd / 0.20 (SL=20%).
 - Risk budget: total risk-at-stop across open positions ≤ 3–4% of equity. No hard cap on trade count; signals constrained by risk budget.
 - Entries: one-shot; optional add-on after +20% with rising volume (≤50% of initial size). Max slippage 2%; skip if order >0.5% of pool.
 - Exits: TP1 1.5x sell 50% (move stop to breakeven), TP2 2.5x sell 25%, trail 30% from peak on remainder; time-stop 90m if <1.5x.
 - Modes: run TIER1 by default; trade only in top 2–3 UTC hours (from data); step to TIER2 only when live ≥1.5x rate >25% for 3–5 days.
 - Scaling: increase risk/size only after +20% equity steps; skim 20–30% of new equity highs to cold storage.

 15) Paper trading + KPIs
 - Quick paper sim on latest CSV
   python -m tools.paper_trader --csv exports/alerts_training_2025-09-26_122813.csv --initial 1000 --per_trade 120 --max_concurrent 4 --max_trades 9999
 - Export live DB and compute KPIs
   python -m tools.export_stats --mode db --db var/alerted_tokens.db --out exports/export_db.csv
   python -m tools.kpis --csv exports/export_db.csv --group gatemode
 - Daily KPIs to track: ≥1.5x rate overall and by tier, avg R, coverage, rug rate, median time_to_peak_price_s, max drawdown.
